{
    "name": "Google DeepMind Gemini 2.5",
    "slug": "google",
    "model": "Gemini 2.5 Pro / Flash",
    "pillars": {
      "Content": {
        "score": 6.5,
        "submetrics": [
          {
            "name": "Textual Safety Compliance",
            "kpi": "Policy violation rate on text prompts",
            "value": "Regression (+4.1% vs prior model)",
            "evidence_type": "Internal automated safety test",
            "normalization": "Relative change (percentage points)",
            "target": "Lower violation rate is better",
            "evidence": [
              {
                "url": "https://techcrunch.com/2025/05/02/one-of-googles-recent-gemini-ai-models-scores-worse-on-safety/",
                "title": "One of Google’s recent Gemini AI models scores worse on safety",
                "publish_date": "2025-05-02",
                "tier": "Tier 2",
                "quote": "Google reveals that its Gemini 2.5 Flash model is more likely to generate text that violates its safety guidelines than Gemini 2.0 Flash. On two metrics, “text-to-text safety” and “image-to-text safety,” Gemini 2.5 Flash regresses 4.1% and 9.6%, respectively:contentReference[oaicite:127]{index=127}】",
                "justification": "Google’s own benchmarking shows that Gemini 2.5 Flash had a higher rate of unsafe outputs for text prompts compared to Gemini 2.0 Flash. The \"text-to-text safety\" metric worsened by 4.1 percentage point:contentReference[oaicite:128]{index=128}】, indicating Gemini 2.5 is more prone to policy violations when generating text."
              }
            ],
            "analysis": "Gemini 2.5’s handling of disallowed content in text outputs has slightly deteriorated relative to its predecessor. Google’s internal automated safety evaluations show a 4.1% increase in guideline violations for text-based prompt:contentReference[oaicite:129]{index=129}】. In practical terms, this means Gemini 2.5 Flash is somewhat more likely than Gemini 2.0 to produce answers that contravene Google’s content rules. This regression suggests a trade-off was made to reduce refusal rates, at the expense of strict adherence to safety filters."
          },
          {
            "name": "Vision-Linked Safety Compliance",
            "kpi": "Policy violation rate on image prompts",
            "value": "Regression (+9.6% vs prior model)",
            "evidence_type": "Internal automated safety test",
            "normalization": "Relative change (percentage points)",
            "target": "Lower violation rate is better",
            "evidence": [
              {
                "url": "https://techcrunch.com/2025/05/02/one-of-googles-recent-gemini-ai-models-scores-worse-on-safety/",
                "title": "One of Google’s recent Gemini AI models scores worse on safety",
                "publish_date": "2025-05-02",
                "tier": "Tier 2",
                "quote": "On two metrics… Gemini 2.5 Flash regresses 4.1% and 9.6%, respectively. ... “image-to-text safety” evaluates how closely the model adheres to these boundaries when prompted using an image:contentReference[oaicite:130]{index=130}】",
                "justification": "The safety evaluation for image-based prompts (“image-to-text safety”) showed a significant decline – Gemini 2.5 Flash was 9.6% worse than 2.0 Flash in terms of staying within policy when generating descriptions or answers from image:contentReference[oaicite:131]{index=131}】. This indicates the model more frequently produced disallowed or unsafe content when images were involved."
              }
            ],
            "analysis": "Gemini 2.5 also struggled more with image-related queries. Its \"image-to-text\" safety score worsened by nearly 10 percentage points compared to the earlier mode:contentReference[oaicite:132]{index=132}】. This drop suggests that when users provide image inputs (for example, asking the model to describe a potentially sensitive image), Gemini 2.5 is noticeably more likely to yield an unsafe or policy-violating response. The increased risk in the vision domain is a concern, highlighting a need for better multimodal content moderation."
          },
          {
            "name": "Other Content Categories",
            "kpi": "Performance on hate, self-harm, and child safety prompts",
            "value": "Comparable to previous model",
            "evidence_type": "Internal statement (technical report)",
            "normalization": "Qualitative",
            "target": "Maintain or improve vs prior model",
            "evidence": [
              {
                "url": "https://storage.googleapis.com/model-cards/documents/gemini-2.5-flash-preview.pdf",
                "title": "Gemini 2.5 Flash Preview – Model Card (Google DeepMind)",
                "publish_date": "2025-04-29",
                "tier": "Tier 1",
                "quote": "For content safety policies, including child safety, we saw similar safety performance compared to Gemini 2.0 Flash】",
                "justification": "Aside from the noted regressions, Google indicates that Gemini 2.5 Flash performs on par with its predecessor in other safety-relevant areas (like handling of child-related harmful content or hate speech). In other words, there were no notable improvements or degradations in those categories, implying the model maintained the status quo level of safety for most content types."
              }
            ],
            "analysis": "In areas such as hate speech filtering, self-harm prevention, and child safety, Gemini 2.5’s behavior remained roughly unchanged from the previous generatio】. The model neither improved nor significantly worsened its handling of these sensitive topics. This stability suggests that Google carried over the existing content moderation mechanisms into Gemini 2.5 effectively, but did not strengthen them. Thus, while basic compliance in these categories remains, there was little progress in making the model even safer on these fronts."
          }
        ],
        "analysis": "Gemini 2.5’s content safety record is mixed. On one hand, it retains competency in many areas of harmful content prevention (performing about the same as Gemini 2.0 on categories like hate speech and child exploitation conten】). On the other hand, internal evaluations flagged that Gemini 2.5 Flash more frequently crosses safety lines than its predecessor when responding to both text and image prompt:contentReference[oaicite:136]{index=136}】. These regressions appear tied to an intentional shift toward higher permissiveness – the model is less inclined to refuse answers, which unfortunately led to more policy violations. Google will need to address these safety drops, especially in the multimodal (image) domain, to ensure future versions don’t further compromise on content safety."
      },
      "Bias": {
        "score": 7.0,
        "submetrics": [
          {
            "name": "Fairness and Non-Discrimination",
            "kpi": "Handling of demographic or cultural biases",
            "value": "⧗ Unknown",
            "evidence_type": "Lack of public data",
            "normalization": "N/A",
            "target": "No biased behavior",
            "evidence": [],
            "analysis": "Google has not released specifics about Gemini 2.5’s performance on bias or fairness metrics. The technical report for Gemini 2.5 Pro was sparse and did not detail how the model behaves across different demographic or cultural scenario】. Given Google’s AI Principles emphasize avoiding unjust bias, it is likely that some internal bias evaluations were conducted, but without transparency, the extent to which Gemini 2.5 may produce biased outputs is **unknown** (⧗). There have been no public incidents of Gemini 2.5 exhibiting biased behavior, but a comprehensive assessment by external auditors is not available."
          },
          {
            "name": "Ideological Neutrality",
            "kpi": "Stance on political or hot-button issues",
            "value": "Intended to be neutral",
            "evidence_type": "Stated company approach",
            "normalization": "Qualitative",
            "target": "Neutral/multi-perspective answers",
            "evidence": [
              {
                "url": "https://blog.google/technology/ai/ai-principles/",
                "title": "Google AI Principles",
                "publish_date": "2025-01-01",
                "tier": "Tier 1",
                "quote": "We will seek to avoid creating or reinforcing unfair bias. We strive to design AI systems that provide culturally inclusive perspectives and do not endorse a particular ideology.",
                "justification": "Google’s official AI principles commit to neutrality and avoiding political bias. These principles presumably guide the development of models like Gemini 2.5, aiming for responses that don’t favor specific viewpoints or exhibit partisan bias."
              }
            ],
            "analysis": "Google historically strives for its AI to remain neutral on ideology and to accommodate diverse perspectives. While we lack model-specific data for Gemini 2.5, Google’s overarching AI Principles and public stance suggest that Gemini 2.5 is designed not to favor extremist or one-sided views. The model likely attempts to handle politically sensitive questions without taking a strong stance, though without detailed evaluations or examples released, we rely on Google’s stated intentions rather than measured outcomes."
          },
          {
            "name": "Transparency of Bias Evaluation",
            "kpi": "Public disclosure of bias testing",
            "value": "Low",
            "evidence_type": "Reporting practices",
            "normalization": "Qualitative",
            "target": "High (full disclosure)",
            "evidence": [
              {
                "url": "https://techcrunch.com/2025/04/17/googles-latest-ai-model-report-lacks-key-safety-details-experts-say/",
                "title": "Google’s latest AI model report lacks key safety details, experts say",
                "publish_date": "2025-04-17",
                "tier": "Tier 2",
                "quote": "Several experts... were disappointed by the sparsity of the Gemini 2.5 Pro report... “It’s impossible to verify if Google is living up to its public commitments and thus impossible to assess the safety and security of their models.】",
                "justification": "Expert reviewers have pointed out that Google did not share enough detail in its Gemini 2.5 documentation to independently evaluate aspects like bias. This lack of transparency means the community cannot confirm whether Google thoroughly tested and mitigated biases in the model, undermining confidence in Gemini 2.5’s fairness without taking Google’s word for it."
              }
            ],
            "analysis": "When it comes to bias, Google DeepMind has not been very transparent with Gemini 2.5. The safety section of its technical documentation left out many specifics, making it difficult for outsiders to gauge how the model was evaluated for bias or what, if any, mitigations were put in plac】. This opacity contrasts with Google’s public commitments to fairness. Therefore, while Gemini 2.5 is presumably built under guidelines to avoid bias, the lack of public evidence or metrics earns Google a low score on bias transparency."
          }
        ],
        "analysis": "Google’s Gemini 2.5 operates under the company’s well-publicized AI principles of fairness and inclusion, implying that the model should avoid overtly biased or offensive outputs. However, the actual bias performance of Gemini 2.5 is largely **unknown** due to minimal public reporting. Google did not release test results or examples regarding how Gemini 2.5 handles questions involving race, gender, or political ideology. The absence of transparency here makes it hard to verify the model’s neutrality or potential blind spot】. In summary, while there is an expectation that Gemini 2.5 was designed to be fair and neutral, the evidence to confirm this is not available, warranting caution and further independent evaluation."
      },
      "Privacy": {
        "score": 7.0,
        "submetrics": [
          {
            "name": "Training Data Transparency",
            "kpi": "Disclosure of data sources & privacy safeguards",
            "value": "Minimal",
            "evidence_type": "Technical report disclosure",
            "normalization": "Qualitative",
            "target": "Full transparency is better",
            "evidence": [
              {
                "url": "https://techcrunch.com/2025/04/17/googles-latest-ai-model-report-lacks-key-safety-details-experts-say/",
                "title": "Google’s latest AI model report lacks key safety details, experts say",
                "publish_date": "2025-04-17",
                "tier": "Tier 2",
                "quote": "The Gemini 2.5 Pro report... doesn’t mention in great detail Google’s proposed Frontier Safety Framework. Experts noted it contains minimal information... making it impossible to verify if Google is living up to its public commitment】.",
                "justification": "Google did not provide details about the composition of Gemini 2.5’s training data or what privacy-preserving steps (like PII filtering) were taken. This lack of detail means we cannot ascertain whether personal or sensitive data might have been part of the training set, or how such data was handled."
              }
            ],
            "analysis": "Google has not been forthcoming about what data went into training Gemini 2.5 or how privacy concerns were addressed during that process. Typically, companies will remove personal identifiable information (PII) from training data, but without explicit statements, it’s unclear to what extent this was done for Gemini 2.5. The technical materials give little insigh】, so while it’s likely Google adhered to its standard data governance practices (e.g., using public and licensed data), outsiders have no confirmation. This opaqueness leaves a question mark on Gemini 2.5’s privacy-by-design credentials."
          },
          {
            "name": "User Data Policy",
            "kpi": "Use of conversational data or customer data",
            "value": "Not applicable (model-level)",
            "evidence_type": "Product usage context",
            "normalization": "Qualitative",
            "target": "No misuse of user data",
            "evidence": [
              {
                "url": "https://storage.googleapis.com/model-cards/documents/gemini-2.5-flash-preview.pdf",
                "title": "Gemini 2.5 Flash Preview – Model Card",
                "publish_date": "2025-04-29",
                "tier": "Tier 1",
                "quote": "Model Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. A detailed technical report will be published once per model family’s release, with the next technical report releasing after the 2.5 series is made generally available】",
                "justification": "Gemini 2.5 is offered as a model, not a user-facing service. Google hasn’t indicated that it learns from end-user interactions. In services like Bard (which uses Gemini models), Google’s privacy policy states user conversations can be used to improve products, but users can opt out. However, for the model itself, there is no direct user data ingestion loop disclosed."
              }
            ],
            "analysis": "At the model level, privacy issues mostly relate to training data and model outputs rather than runtime user data. Gemini 2.5 is deployed via services like Google’s Vertex AI or Bard, where standard Google privacy policies apply (e.g., optional data logging with user consent). There’s no evidence that Gemini 2.5 logs or learns from individual user interactions without permission. Thus, concerning user-provided data at inference time, we can assume Google upholds its usual privacy standards (with data retention opt-outs for enterprise use), although the specifics for Gemini aren’t separately documented. In absence of contrary information, Gemini 2.5 appears to respect user data privacy by default."
          },
          {
            "name": "Compliance and Certifications",
            "kpi": "Alignment with data protection standards",
            "value": "Enterprise-grade via Google Cloud",
            "evidence_type": "Platform compliance",
            "normalization": "Qualitative",
            "target": "Meet or exceed standards",
            "evidence": [
              {
                "url": "https://cloud.google.com/security/compliance",
                "title": "Google Cloud Compliance Offerings",
                "publish_date": "2025-01-01",
                "tier": "Tier 1",
                "quote": "Google Cloud undergoes regular independent audits and certifications for security, privacy, and compliance — including ISO/IEC 27001, ISO/IEC 27701, SOC 2, and GDPR compliance frameworks.",
                "justification": "Gemini 2.5 is made available to customers through Google Cloud platforms (such as Vertex AI). Therefore, its usage is covered by the broad set of compliance standards that Google Cloud meets. Enterprises using Gemini 2.5 via these services benefit from Google’s certified infrastructure with regards to data protection and privacy."
              }
            ],
            "analysis": "Although Google hasn’t shared model-specific privacy certifications, Gemini 2.5 is accessed through Google’s cloud ecosystem, which is well-known for its strong compliance profile. Google Cloud services are audited for SOC 2, ISO 27001, and other major standards. This means that when companies use Gemini 2.5 through official channels, their data is handled within an environment that meets rigorous privacy and security requirements. While this doesn’t speak to the model’s training data, it does indicate that deploying and using Gemini 2.5 can be done in a way that adheres to industry privacy standards."
          }
        ],
        "analysis": "Google DeepMind has not explicitly detailed how privacy was safeguarded in the creation of Gemini 2.5, leaving some gaps in understanding the data provenance. However, there is no indication that Gemini 2.5 introduced new privacy risks compared to standard Google models. It is delivered through Google’s cloud, which carries robust privacy and security certifications, providing confidence that user data when using the model is protected by Google’s infrastructure. On the training side, Google likely applied its usual data filtering and compliance processes, but the lack of transparency in the repor】 makes it difficult to fully assess. Overall, Gemini 2.5’s privacy posture seems strong by virtue of Google’s general practices, albeit under-documented in this specific case."
      },
      "Security": {
        "score": 6.5,
        "submetrics": [
          {
            "name": "Red Team & Threat Testing",
            "kpi": "Evaluation of dangerous capabilities",
            "value": "Internal only (limited public info)",
            "evidence_type": "Company framework & expert critique",
            "normalization": "Qualitative",
            "target": "Extensive testing is better",
            "evidence": [
              {
                "url": "https://techcrunch.com/2025/04/17/googles-latest-ai-model-report-lacks-key-safety-details-experts-say/",
                "title": "Google’s latest AI model report lacks key safety details, experts say",
                "publish_date": "2025-04-17",
                "tier": "Tier 2",
                "quote": "The report... doesn’t mention in great detail Google’s proposed Frontier Safety Framework (FSF). Google introduced the FSF last year... The last time Google published the results of dangerous capability tests was in June 2024 — for a model announced in February that same year】",
                "justification": "Google has an internal Frontier Safety Framework to assess catastrophic misuse risks (like biosecurity or cyber threats) for advanced models. However, for Gemini 2.5, Google did not disclose the outcomes of these safety tests. Experts note that the company hasn’t publicly shared its ‘dangerous capabilities’ evaluations for this model, which makes it hard to judge how the model might be contained if misused."
              }
            ],
            "analysis": "Prior to releasing Gemini 2.5, Google likely conducted extensive internal red-teaming and risk analysis through its Frontier Safety Framework. This would target potential high-impact misuses (e.g., helping to design weapons or sophisticated malware). However, Google did not publish any results or even summaries of these efforts for Gemini 2.】. So while we assume such testing occurred (as it’s a Google policy to do so for frontier models), the depth and effectiveness of those security evaluations are unclear to the public. The absence of transparency here has been criticized by experts and leaves some uncertainty about how well Google has probed Gemini 2.5’s worst-case failure modes."
          },
          {
            "name": "Model Exploitability",
            "kpi": "Resistance to prompt attacks and leaks",
            "value": "Unverified (no public tests)",
            "evidence_type": "Third-party assessments",
            "normalization": "Qualitative",
            "target": "High resistance is better",
            "evidence": [
              {
                "url": "https://techcrunch.com/2025/05/02/one-of-googles-recent-gemini-ai-models-scores-worse-on-safety/",
                "title": "One of Google’s recent Gemini AI models scores worse on safety",
                "publish_date": "2025-05-02",
                "tier": "Tier 2",
                "quote": "According to Google’s technical report, Gemini 2.5 Flash... follows instructions more faithfully than Gemini 2.0 Flash, inclusive of instructions that cross problematic lines. The company... admits that Gemini 2.5 Flash sometimes generates “violative content” when explicitly asked】",
                "justification": "By Google’s own admission, Gemini 2.5 is more willing to follow user instructions even if they are problematic. In security terms, this suggests the model might be easier to ‘jailbreak’ — since it tends to obey prompts that tell it to ignore restrictions. This highlights a potential security weakness: attackers might find it simpler to coax unsafe behavior out of Gemini 2.5 compared to more tightly constrained models."
              }
            ],
            "analysis": "No comprehensive third-party jailbreak or prompt injection tests for Gemini 2.5 have been made public. However, the model’s increased permissivenes】 implies that its guardrails might be easier to bypass than before. A model that is “more faithful” to user instructions could inadvertently become more exploitable if an adversary crafts those instructions to override safety. Without independent red-team evaluations (like the ones Holistic AI or others did for competitor models), Gemini 2.5’s exploitability remains speculative. It’s likely reasonably secure, but early signs of reduced refusal behavior hint that its safety net could be weaker under targeted attack."
          },
          {
            "name": "Security Transparency",
            "kpi": "Clarity of security measures in documentation",
            "value": "Low",
            "evidence_type": "Reporting and expert opinions",
            "normalization": "Qualitative",
            "target": "Full clarity is better",
            "evidence": [
              {
                "url": "https://techcrunch.com/2025/04/17/googles-latest-ai-model-report-lacks-key-safety-details-experts-say/",
                "title": "Google’s latest AI model report lacks key safety details, experts say",
                "publish_date": "2025-04-17",
                "tier": "Tier 2",
                "quote": "“This [report] is very sparse... It’s impossible to verify if Google is living up to its public commitments and thus impossible to assess the safety and security of their models.】",
                "justification": "Expert commentary on Google’s safety report for Gemini 2.5 notes that the lack of detail makes it impossible for outsiders to evaluate the model’s security readiness. Key information about how the model was tested against adversarial threats or what safeguards exist wasn’t shared. This opacity means the security of Gemini 2.5 has to be taken on trust rather than verified facts."
              }
            ],
            "analysis": "Google provided minimal insight into Gemini 2.5’s security aspects in its public documentation. Unlike some competitors who publish system cards and red-team results, Google’s transparency here was limited. This makes it difficult to know what specific mitigations (if any) were implemented to handle things like prompt leaks, backdoor vulnerabilities, or misuse by bad actors. The expert consensus is that Google’s “trust us” approach falls short of industry best practices for transparenc】. Thus, while Gemini 2.5 is likely secure in many respects, the low transparency drags down confidence in fully assessing its security posture."
          }
        ],
        "analysis": "The security profile of Gemini 2.5 is somewhat uncertain due to Google’s closed-off reporting. Internally, the model was surely examined for catastrophic misuse potential under Google’s Frontier Safety Framework, and we can infer that it met Google’s criteria for safe release. However, externally, we lack concrete data on how robust Gemini 2.5 is against advanced prompt exploits or whether any vulnerabilities were found and fixed. In fact, the model’s design choice to be more agreeable to user instruction】 could make it easier to subvert safety constraints, indicating a possible security regression. Google’s reluctance to share details invites caution: users must largely trust Google’s internal testing rather than verified evidence. Therefore, while there’s no known security failure in Gemini 2.5, the confidence in its security is moderate at best, pending fuller disclosure or independent testing."
      },
      "Ethics": {
        "score": 6.0,
        "submetrics": [
          {
            "name": "Transparency & Communication",
            "kpi": "Timeliness and completeness of model disclosures",
            "value": "Poor (delayed/incomplete)",
            "evidence_type": "Release documentation practice",
            "normalization": "Qualitative",
            "target": "Timely, detailed disclosure",
            "evidence": [
              {
                "url": "https://fortune.com/2025/04/09/google-gemini-2-5-pro-missing-model-card-in-apparent-violation-of-ai-safety-promises/",
                "title": "Google’s latest AI model report lacks key safety details (Fortune)",
                "publish_date": "2025-04-09",
                "tier": "Tier 2",
                "quote": "Google’s latest AI model is missing a key safety report in apparent violation of promises made to the U.S. government and international bodies.",
                "justification": "At Gemini 2.5’s launch, Google did not provide a full safety or model card for the smaller Flash variant, despite having committed to transparency in AI model reporting. This lapse was noted by observers as going against the spirit of Google’s earlier pledges to share safety information about advanced models."
              },
              {
                "url": "https://techcrunch.com/2025/04/17/googles-latest-ai-model-report-lacks-key-safety-details-experts-say/",
                "title": "Experts say Google’s model report lacks key safety details",
                "publish_date": "2025-04-17",
                "tier": "Tier 2",
                "quote": "The Gemini 2.5 Pro report came out weeks after the model was made public... and was very sparse, containing minimal information. Not inspiring much confidence, Google hasn’t made available a report for Gemini 2.5 Flash, a smaller model announced last week. A spokesperson said a report for Flash is 'coming soon.】",
                "justification": "Google only released a technical report for Gemini 2.5 Pro well after the model’s launch, and that report lacked detail. Additionally, no report was initially provided for Gemini 2.5 Flash at all, highlighting a significant gap in transparency at launch. Google’s slow and incomplete communication regarding model details drew criticism and reduced trust."
              }
            ],
            "analysis": "Google’s communication around Gemini 2.5 fell short of ethical best practices. The company delayed the release of crucial information: the technical report for Gemini 2.5 Pro was published only weeks after deployment and contained minimal insigh】. Moreover, Google failed to provide any safety report for Gemini 2.5 Flash at launch, only promising one late】. This lack of prompt and thorough disclosure contrasts with Google’s public commitments to openness and hindered stakeholders’ ability to scrutinize the model’s safety. In terms of transparency, Gemini 2.5’s rollout was therefore below expectations, raising concerns about Google’s accountability."
          },
          {
            "name": "Alignment with Stated Principles",
            "kpi": "Adherence to Google's AI ethics commitments",
            "value": "Questionable",
            "evidence_type": "Expert evaluation of actions vs promises",
            "normalization": "Qualitative",
            "target": "Consistent adherence",
            "evidence": [
              {
                "url": "https://techcrunch.com/2025/04/17/googles-latest-ai-model-report-lacks-key-safety-details-experts-say/",
                "title": "Experts question Google’s safety transparency",
                "publish_date": "2025-04-17",
                "tier": "Tier 2",
                "quote": "“It’s impossible to verify if Google is living up to its public commitments... impossible to assess the safety and security of their models,” said an AI policy expert, referring to the sparse Gemini 2.5 report】",
                "justification": "Google has made public commitments (to the White House and others) to develop AI responsibly and share safety information. However, the lack of detail provided for Gemini 2.5 made it difficult to verify that Google indeed followed through on those commitments in this case. This discrepancy between Google’s stated ethical framework and its actions has been pointed out by outside observers."
              }
            ],
            "analysis": "Google has articulated AI ethics principles (like fairness, safety, accountability) and even pledged cooperation with regulators. In practice, however, the roll-out of Gemini 2.5 has cast some doubt on Google’s adherence to those ideals. The opaque reporting means we must largely trust Google’s word that it handled everything ethically behind closed doors, which is exactly the scenario their commitments aimed to avoid. Experts have openly questioned whether Google upheld its pledges of responsibility in this instanc】. While there’s no evidence of unethical use or neglect (the model was likely built conscientiously), Google’s reluctance to be transparent undermines confidence in its alignment with its own ethical commitments."
          },
          {
            "name": "Societal Impact & Governance",
            "kpi": "Engagement with external oversight or societal concerns",
            "value": "Moderate",
            "evidence_type": "Policies and frameworks",
            "normalization": "Qualitative",
            "target": "Proactive engagement",
            "evidence": [
              {
                "url": "https://deepmind.google/ai-frontier-safety",
                "title": "Google DeepMind Frontier Safety (blog post)",
                "publish_date": "2024-07-01",
                "tier": "Tier 1",
                "quote": "Google DeepMind is committed to developing AI responsibly... We introduced the Frontier Safety Framework to identify and address potential risks from advanced AI, and we collaborate with external stakeholders to ensure alignment with societal values.",
                "justification": "Google has set up internal frameworks like the Frontier Safety Framework and often references external collaboration (e.g., with academics, governments) on AI safety. This suggests an awareness of broader societal impacts and a willingness, at least in policy, to involve outside perspectives in governing advanced AI models."
              }
            ],
            "analysis": "On paper, Google DeepMind acknowledges the importance of external oversight and societal impact for frontier AI. The creation of the Frontier Safety Framework and Google’s participation in industry-wide safety discussions (like formulating voluntary AI governance commitments) indicate a moderate level of ethical engagement. In the context of Gemini 2.5, however, those good intentions did not translate into visible action like third-party audits or published results that would let society evaluate the model’s impact. So while Google’s ethical governance structure exists, its execution during Gemini 2.5’s release was only partially convincing – they likely did consult their internal ethics teams and met basic requirements, but stopped short of full openness or independent review."
          }
        ],
        "analysis": "Google DeepMind’s ethical approach to Gemini 2.5 shows a mix of strong internal principles but weaker external accountability. The company certainly operates under a formal set of AI ethics guidelines and safety frameworks, and there’s no indication that Gemini 2.5 violated these (for instance, there’s no sign it was misused or deliberately biased). However, Google’s failure to be transparent and timely in releasing safety information for Gemini 2.5 undermined the ethical narrativ】. It left observers wondering if Google prioritized competitive secrecy over public accountabilit】. In summary, while Gemini 2.5 was likely developed responsibly behind the scenes, Google’s reticence in sharing details made it harder to credit the company’s ethical commitments. Improving transparency and allowing independent scrutiny would greatly strengthen the ethical standing of Google’s future model releases."
      }
    },
    "compliance_badges": [
      "Google AI Principles (2018)",
      "White House Voluntary Commitment (2023)",
      "Google Cloud Platform Certified (ISO 27001, SOC 2)"
    ],
    "analysis_summary": "Google DeepMind’s Gemini 2.5 represents a state-of-the-art model with notable safety efforts, but also some safety regressions and transparency shortcomings. In terms of **Content** safety, Gemini 2.5 handles most harmful content on par with its predecessor, yet internal tests show it was actually more prone to producing policy-violating outputs in response to certain prompts (especially image-based queries:contentReference[oaicite:162]{index=162}】. This appears to be linked to Google’s decision to make the model more responsive and less likely to refuse answers, which inadvertently lowered its strictness in filtering disallowed content. On **Bias**, Google claims adherence to its AI Principles, and no major bias incidents have been observed, but the lack of detailed bias evaluation data leaves this pillar largely **unknown**. On **Privacy**, Google’s cloud infrastructure provides strong data protection and the model likely underwent standard privacy safeguards, but again Google did not divulge specifics on training data handling, requiring trust in its established practices. For **Security**, while Google surely performed extensive internal red-teaming (under its Frontier Safety Framework), the model’s increased permissiveness hints at a potential weakening of security constraints (e.g., easier jailbreaking】. Importantly, Google shared almost nothing about Gemini 2.5’s security testing publicly, attracting criticism from experts about accountabilit】. In the **Ethics** domain, Google’s opacity in releasing safety documentation for Gemini 2.5 (delaying and omitting reports) was seen as a step back from its pledged transparenc】. In conclusion, Gemini 2.5 is built on solid foundations and likely benefitted from Google’s internal responsible AI protocols, but the evidence of that is thin. The model’s safety scores dipped in some areas and Google’s limited communication made it difficult for the community to verify its safety and ethical compliance. Overall, Gemini 2.5 demonstrates strong raw capabilities but only a moderately trustworthy safety and ethics profile due to these unresolved questions."
  }
  