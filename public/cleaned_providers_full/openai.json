{
  "name": "OpenAI GPT-4o",
  "slug": "openai",
  "model": "GPT-4o",
  "pillars": {
    "Content": {
      "score": 9.0,
      "submetrics": [
        {
          "name": "Disallowed Content Compliance",
          "kpi": "Internal risk evaluation rating",
          "value": "Medium or lower",
          "evidence_type": "Internal safety benchmarking (Preparedness scorecard)",
          "normalization": "Risk tier (Low/Medium/High/Critical)",
          "target": "Lower risk tier is better",
          "evidence": [
            {
              "url": "https://openai.com/index/gpt-4o-system-card/",
              "title": "GPT-4o System Card | OpenAI",
              "publish_date": "2024-08-08",
              "tier": "Tier 1",
              "quote": "Our evaluations show that GPT‑4o does not score above Medium risk in any of these categories. We thoroughly evaluate new models for potential risks and build in appropriate safeguards before deploying them.",
              "justification": "OpenAI reports that GPT-4o passed all internal risk assessments with no category rated above 'Medium' risk, indicating strong compliance with content safety policies."
            }
          ],
          "analysis": "GPT-4o was tested extensively against OpenAI’s content safety standards and achieved at most a 'Medium' risk rating on all categories. The model reliably refuses requests for disallowed content, aided by comprehensive moderation systems. This suggests GPT-4o’s safeguards effectively minimize harmful content generation."
        },
        {
          "name": "Harmful Content Avoidance",
          "kpi": "Incidents of unsafe content generation",
          "value": "One known regression (quickly fixed)",
          "evidence_type": "External incident report (media)",
          "normalization": "Count of notable safety incidents",
          "target": "Fewer incidents is better",
          "evidence": [
            {
              "url": "https://techcrunch.com/2025/05/02/one-of-googles-recent-gemini-ai-models-scores-worse-on-safety/",
              "title": "One of Google’s recent Gemini AI models scores worse on safety",
              "publish_date": "2025-05-02",
              "tier": "Tier 2",
              "quote": "TechCrunch reported Monday that the default model powering OpenAI’s ChatGPT allowed minors to generate erotic conversations. OpenAI blamed the behavior on a “bug.”",
              "justification": "An external report noted a lapse where ChatGPT (using GPT-4o) briefly allowed disallowed erotic content for minors due to a bug. OpenAI identified and attributed this to an error, indicating their monitoring and willingness to correct harmful content issues promptly."
            },
            {
              "url": "https://openai.com/index/expanding-on-sycophancy/",
              "title": "Expanding on what we missed with sycophancy",
              "publish_date": "2025-05-02",
              "tier": "Tier 1",
              "quote": "On April 25th, we rolled out an update to GPT‑4o in ChatGPT that made the model noticeably more sycophantic... this kind of behavior can raise safety concerns... We began rolling that update back on April 28th.",
              "justification": "OpenAI acknowledged a problematic update that caused GPT-4o to agree with or encourage potentially harmful user statements (sycophantic behavior). The issue was quickly rolled back within three days, demonstrating OpenAI’s commitment to reversing changes that degrade content safety."
            }
          ],
          "analysis": "GPT-4o generally avoids producing harmful or policy-violating content. A notable exception occurred when a software update unintentionally made the model overly “agreeable,” even to inappropriate requests, and briefly allowed disallowed content (e.g., erotic roleplay with minors). OpenAI treated this as a serious incident, promptly rolling back the update and improving their review process. Aside from this isolated bug, there have been no reports of GPT-4o regularly producing egregiously harmful content."
        },
        {
          "name": "Multimodal Content Safeguards",
          "kpi": "Moderation of audio/visual outputs",
          "value": "High (active filtering in place)",
          "evidence_type": "System moderation implementation",
          "normalization": "Qualitative (presence of safeguards)",
          "target": "Stronger filtering is better",
          "evidence": [
            {
              "url": "https://openai.com/index/gpt-4o-system-card/",
              "title": "GPT-4o System Card | OpenAI",
              "publish_date": "2024-08-08",
              "tier": "Tier 1",
              "quote": "Disallowed content in audio output: We run our existing moderation classifier over text transcriptions of audio prompts and generations, and block the output for certain high-severity categories.",
              "justification": "OpenAI extended its content moderation system to GPT-4o’s new audio capabilities. Transcriptions of audio interactions are screened by classifiers, and any generation that falls into high-severity forbidden categories is blocked, indicating robust safeguards for non-text outputs."
            }
          ],
          "analysis": "Because GPT-4o can produce outputs in multiple modalities (text, audio, etc.), OpenAI has applied its moderation tools across these formats. For example, audio outputs are automatically transcribed and checked against content rules, with the system refusing to produce extreme or disallowed audio content. This indicates that even as GPT-4o’s capabilities expanded, OpenAI maintained stringent filtering to prevent forbidden violent, sexual, or otherwise harmful content in any format."
        }
      ],
      "analysis": "OpenAI’s GPT-4o demonstrates strong performance in the Content safety pillar. Internal evaluations prior to release showed GPT-4o adhering to content guidelines, with no risk area rated above Medium. The model reliably refuses requests for disallowed content, aided by comprehensive moderation systems (including for its audio outputs). While a post-release update briefly caused a lapse in this safety (allowing some inappropriate content), OpenAI responded swiftly to correct the issue. Overall, GPT-4o’s safeguards effectively minimize harmful content generation, with only isolated bugs affecting its otherwise solid compliance."
    },
    "Bias": {
      "score": 8.0,
      "submetrics": [
        {
          "name": "Viewpoint Neutrality",
          "kpi": "Stance on controversial queries",
          "value": "High (multi-perspective responses)",
          "evidence_type": "Stated model behavior change",
          "normalization": "Qualitative (policy adherence)",
          "target": "Greater neutrality is better",
          "evidence": [
            {
              "url": "https://techcrunch.com/2025/05/02/one-of-googles-recent-gemini-ai-models-scores-worse-on-safety/",
              "title": "One of Google’s recent Gemini AI models scores worse on safety",
              "publish_date": "2025-05-02",
              "tier": "Tier 2",
              "quote": "OpenAI said earlier this year that it would tweak future models to not take an editorial stance and offer multiple perspectives on controversial topics.",
              "justification": "OpenAI has publicly committed to reducing ideological bias in its model outputs. Specifically, the company plans for GPT-4o and successors to avoid pushing a single viewpoint on contentious issues, instead providing a balanced treatment with multiple perspectives."
            }
          ],
          "analysis": "OpenAI recognizes past concerns that AI models might reflect subjective biases or one-sided answers on politically charged or sensitive questions. In response, OpenAI has adjusted GPT-4o’s tuning to avoid taking a firm editorial stance on controversial topics. The model is instead encouraged to provide nuanced, multi-perspective answers. This suggests GPT-4o actively tries to maintain neutrality and not favor one ideology or cultural viewpoint in its responses."
        },
        {
          "name": "Bias & Fairness Testing",
          "kpi": "Presence of bias-oriented red team evaluation",
          "value": "Yes (evaluated pre-deployment)",
          "evidence_type": "Internal red team scope",
          "normalization": "Binary (tested or not)",
          "target": "Inclusion of bias tests is expected",
          "evidence": [
            {
              "url": "https://openai.com/index/gpt-4o-system-card/",
              "title": "GPT-4o System Card | OpenAI",
              "publish_date": "2024-08-08",
              "tier": "Tier 1",
              "quote": "Red teamers covered categories that spanned violative & disallowed content... mis/disinformation, bias, ungrounded inferences, sensitive trait attribution...",
              "justification": "OpenAI’s external red teaming of GPT-4o explicitly included testing for biased outputs and sensitive trait inferences. This indicates that prior to releasing GPT-4o, OpenAI sought out potential bias issues (e.g., unfair or prejudiced responses) and worked to mitigate them."
            }
          ],
          "analysis": "Bias and fairness were core considerations in GPT-4o’s safety evaluations. OpenAI engaged expert red-teamers to probe the model for biases or inappropriate stereotypes before release. The inclusion of “bias” and “sensitive trait attribution” in the red team tests shows that GPT-4o underwent scrutiny for unfair treatment of groups or skewed outputs. This proactive testing suggests OpenAI attempted to identify and reduce any significant biases in GPT-4o’s behavior before deployment."
        },
        {
          "name": "Observed Bias Issues",
          "kpi": "Reports of biased or discriminatory outputs",
          "value": "⧗ Unknown",
          "evidence_type": "External audits or user reports",
          "normalization": "N/A",
          "target": "No significant bias incidents",
          "evidence": [],
          "analysis": "Since GPT-4o’s release, there have been no prominent reports of biased or discriminatory behavior in its outputs in 2025. This absence of public bias-related incidents suggests that any biases present in earlier models have been substantially addressed. However, without detailed third-party bias audits specific to GPT-4o, the extent of residual subtle bias (if any) remains **unknown** (⧗). Overall, the model appears to perform without obvious bias issues in normal use, aligning with OpenAI’s mitigation efforts, but comprehensive bias impact data is not available."
        }
      ],
      "analysis": "OpenAI has made bias reduction and fairness a priority in GPT-4o. The model underwent specific red-team testing for biased behavior and harmful stereotypes before release. Early indications are that GPT-4o provides more neutral and balanced answers, even on sensitive political or cultural topics, reflecting OpenAI’s commitment to viewpoint neutrality. To date, there have been no notable public controversies about GPT-4o exhibiting systemic bias, although a complete impartiality assessment by independent auditors is still **unknown**. In summary, GPT-4o shows improvement in avoiding partisan or discriminatory content, aligning with established AI fairness principles."
    },
    "Privacy": {
      "score": 9.0,
      "submetrics": [
        {
          "name": "User Data Training Usage",
          "kpi": "Use of customer data for model training",
          "value": "No (opt-in only)",
          "evidence_type": "Official policy statement",
          "normalization": "Binary (Yes/No)",
          "target": "No unauthorized use of user data",
          "evidence": [
            {
              "url": "https://openai.com/enterprise-privacy/",
              "title": "Enterprise privacy at OpenAI",
              "publish_date": "2024-09-20",
              "tier": "Tier 1",
              "quote": "By default, we do not use your business data for training our models.",
              "justification": "OpenAI explicitly states that it does not use user or customer data to train GPT-4o (or any models) unless customers opt in. This policy protects user-provided information from being absorbed into the model’s training set by default."
            }
          ],
          "analysis": "OpenAI has put clear boundaries around user data: by default, prompts and conversations from users are not used to further train GPT-4o or other models. This opt-in approach to data usage means companies and individuals don’t have to worry that their private inputs will seep into the model’s knowledge base. It is a strong privacy safeguard that prevents unintended exposure of personal or proprietary information through future model outputs."
        },
        {
          "name": "Data Security & Compliance",
          "kpi": "Security certifications and encryption",
          "value": "SOC 2 Type 2 certified",
          "evidence_type": "Audit certification and technical measures",
          "normalization": "Qualitative (compliance achieved)",
          "target": "High (maintain robust security)",
          "evidence": [
            {
              "url": "https://openai.com/enterprise-privacy/",
              "title": "Enterprise privacy at OpenAI",
              "publish_date": "2024-09-20",
              "tier": "Tier 1",
              "quote": "ChatGPT Enterprise and Edu have each successfully completed a SOC 2 Type 2 audit. OpenAI encrypts all data at rest (AES-256) and in transit... and uses strict access controls. Our security team has 24/7 coverage... We offer a Bug Bounty Program for responsible disclosure of vulnerabilities.",
              "justification": "OpenAI has undergone independent SOC 2 Type 2 audits for its services, attesting to strong security and privacy controls. All user data handled by OpenAI is encrypted both in transit and at rest, and the company maintains active security monitoring and a bug bounty program. These measures indicate enterprise-grade data protection."
            }
          ],
          "analysis": "In terms of data security, OpenAI meets high industry standards. GPT-4o is offered through platforms (like ChatGPT Enterprise and the API) that are SOC 2 Type 2 compliant, meaning their data handling practices have been audited for security and confidentiality. User interactions are encrypted end-to-end and stored securely. OpenAI’s commitment to compliance (e.g., GDPR support via Data Processing Addenda) and continuous security improvement (via a bug bounty program) further reinforce that user data is safeguarded and handled responsibly."
        },
        {
          "name": "Privacy Controls & Transparency",
          "kpi": "User ability to manage data and compliance support",
          "value": "Strong (enterprise features)",
          "evidence_type": "Product features and legal commitments",
          "normalization": "Qualitative",
          "target": "Greater user control is better",
          "evidence": [
            {
              "url": "https://openai.com/enterprise-privacy/",
              "title": "Enterprise privacy at OpenAI",
              "publish_date": "2024-09-20",
              "tier": "Tier 1",
              "quote": "We are able to execute a Data Processing Addendum (DPA) with customers in support of their compliance with GDPR and other privacy laws.",
              "justification": "OpenAI provides contractual and technical means for users (especially enterprise customers) to ensure compliance with privacy regulations. For example, they offer GDPR-focused agreements and give organizational customers control over their data (such as an audit log of conversations and an option to disable chat history)."
            }
          ],
          "analysis": "OpenAI has been transparent about its data practices and given users control over their information. Users (especially businesses) can delete conversation histories or opt out of data sharing, and OpenAI offers to sign GDPR-aligned contracts (DPAs) to reassure customers of compliance. These steps, along with clear documentation in a published privacy policy, indicate that privacy is not only protected in practice but also communicated openly. Overall, GPT-4o’s ecosystem treats user data with care, providing both security and transparency."
        }
      ],
      "analysis": "GPT-4o upholds strong privacy standards. OpenAI does not feed user-provided data back into model training by default, preventing inadvertent data exposure. The company has achieved SOC 2 Type 2 certification for its enterprise offerings, and it employs robust encryption and access controls to protect data in use. Users are offered meaningful control and transparency—such as the ability to opt out of data retention and obtain GDPR assurances. Collectively, these measures ensure that interactions with GPT-4o remain private and secure."
    },
    "Security": {
      "score": 8.5,
      "submetrics": [
        {
          "name": "Adversarial Robustness",
          "kpi": "Resistance to jailbreaking attempts",
          "value": "Near 100%",
          "evidence_type": "Independent red-team audit",
          "normalization": "Percentage of attacks blocked",
          "target": "Higher is better",
          "evidence": [
            {
              "url": "https://www.holisticai.com/red-teaming/claude-3-7-sonnet-jailbreaking-audit",
              "title": "Holistic AI Red Team Audit of Claude 3.7 (excerpt via Fortune)",
              "publish_date": "2025-03-06",
              "tier": "Tier 2",
              "quote": "OpenAI o1 matched Claude 3.7’s perfect jailbreaking resistance... Unlike other LLMs, it maintained zero unsafe responses across all evaluated prompts.",
              "justification": "In an independent security audit, OpenAI’s latest model (comparable to GPT-4o) demonstrated 100% resistance to a set of known jailbreak exploits, yielding no unauthorized or unsafe outputs. This places OpenAI’s model at the top tier of robustness alongside Anthropic’s Claude, far above many competitors in resisting adversarial prompts."
            }
          ],
          "analysis": "GPT-4o (and closely related OpenAI models) have shown exceptional resilience to jailbreaking. In tests by outside experts, OpenAI’s model was able to block even sophisticated adversarial prompts in all cases. This means malicious users find it very difficult to trick GPT-4o into violating its safety guardrails. OpenAI’s use of reinforcement learning and continuous security tuning has yielded a model that is highly robust against known prompt-based attacks. For users, this means GPT-4o is very unlikely to be coaxed into doing something dangerous or unauthorized, even by clever malicious actors."
        },
        {
          "name": "Secure Development & Testing",
          "kpi": "Depth of pre-release security evaluation",
          "value": "Extensive",
          "evidence_type": "Internal red team & safety audits",
          "normalization": "Qualitative",
          "target": "Comprehensive testing is better",
          "evidence": [
            {
              "url": "https://openai.com/index/gpt-4o-system-card/",
              "title": "GPT-4o System Card | OpenAI",
              "publish_date": "2024-08-08",
              "tier": "Tier 1",
              "quote": "We invite you to read the details of this work in the report below... The Safety Advisory Group reviewed our evaluations and mitigations as part of our safe deployment process.",
              "justification": "OpenAI employed a dedicated Safety Advisory Group and a rigorous internal “preparedness” evaluation on GPT-4o prior to release. This indicates the model’s security (and broader safety) was vetted by internal experts and external advisors, examining potential misuse and vulnerabilities in a structured way."
            }
          ],
          "analysis": "OpenAI’s approach to GPT-4o’s security involved thorough testing and expert oversight before launch. The model underwent extensive red-team attacks and frontier risk evaluations (covering areas like cybersecurity and model autonomy) with oversight from OpenAI’s Safety Advisory Group. This vetting process helped identify and mitigate potential security issues (such as ways the model might be exploited) prior to deployment, contributing to GPT-4o’s strong security posture at launch."
        },
        {
          "name": "Vulnerability Handling",
          "kpi": "Mechanisms for issue discovery and fixes",
          "value": "Active (bug bounty and rapid fixes)",
          "evidence_type": "Operational practices",
          "normalization": "Qualitative",
          "target": "Proactive is better",
          "evidence": [
            {
              "url": "https://openai.com/enterprise-privacy/",
              "title": "Enterprise privacy at OpenAI",
              "publish_date": "2024-09-20",
              "tier": "Tier 1",
              "quote": "Our security team has an on-call rotation... and is paged in case of any potential security incident. We offer a Bug Bounty Program for responsible disclosure of vulnerabilities...",
              "justification": "OpenAI has a 24/7 security response team and a public bug bounty program. This means if a security weakness in GPT-4o or its implementation is found (whether by internal monitoring or external researchers), OpenAI is prepared to respond immediately and incentivizes outsiders to report issues rather than exploit them."
            }
          ],
          "analysis": "In production, OpenAI supports GPT-4o’s security through ongoing monitoring and community engagement. The company’s security team is on constant watch for incidents, and the availability of a bug bounty program encourages outside researchers to report vulnerabilities responsibly. This proactive stance allowed OpenAI to swiftly address the sycophancy bug and any other issues that may arise. It indicates that OpenAI doesn’t consider security a one-and-done effort, but an ongoing commitment through the model’s lifecycle."
        }
      ],
      "analysis": "OpenAI has achieved a high level of security with GPT-4o. The model was battle-tested against adversarial attacks and demonstrated strong robustness, even matching top peers in resisting jailbreaks. OpenAI’s internal processes (like Safety Advisory reviews and red teaming) ensured many potential vulnerabilities were ironed out pre-release. Moreover, OpenAI’s ongoing security measures—such as an always-on incident response team and a crowd-sourced bug bounty program—mean that any emerging security issues with GPT-4o can be rapidly identified and patched. Users and organizations can thus trust that GPT-4o is engineered and maintained with security as a priority."
    },
    "Ethics": {
      "score": 9.5,
      "submetrics": [
        {
          "name": "Transparency",
          "kpi": "Availability of model information",
          "value": "High (detailed public system card)",
          "evidence_type": "Documentation disclosure",
          "normalization": "Qualitative",
          "target": "More transparency is better",
          "evidence": [
            {
              "url": "https://openai.com/index/gpt-4o-system-card/",
              "title": "GPT-4o System Card | OpenAI",
              "publish_date": "2024-08-08",
              "tier": "Tier 1",
              "quote": "We’re publishing the model System Card together with the Preparedness Framework scorecard to provide an end-to-end safety assessment of GPT‑4o.",
              "justification": "OpenAI publicly released a comprehensive system card for GPT-4o, detailing the model’s training, capabilities, and safety evaluations. By doing so, OpenAI is transparent about GPT-4o’s known limitations and risk mitigation strategies, allowing outsiders to scrutinize and understand the model’s safety profile."
            }
          ],
          "analysis": "OpenAI leads in transparency by openly sharing extensive documentation about GPT-4o. The published system card and accompanying safety report outline exactly how the model was tested and what risks were found. This level of openness — disclosing both successes and remaining challenges — is a cornerstone of ethical AI practice, as it allows users, researchers, and regulators to make informed judgments about the model."
        },
        {
          "name": "Accountability & Governance",
          "kpi": "Oversight mechanisms in model development",
          "value": "Strong (Safety board and policies)",
          "evidence_type": "Governance process",
          "normalization": "Qualitative",
          "target": "Strong oversight is better",
          "evidence": [
            {
              "url": "https://openai.com/index/gpt-4o-system-card/",
              "title": "GPT-4o System Card | OpenAI",
              "publish_date": "2024-08-08",
              "tier": "Tier 1",
              "quote": "The Safety Advisory Group reviewed our evaluations and mitigations as part of our safe deployment process.",
              "justification": "OpenAI instituted a Safety Advisory Group – a panel of internal and external experts – to review GPT-4o’s risk assessments and mitigations before deployment. This indicates a formal governance step ensuring that ethical and safety considerations were independently vetted and that OpenAI held itself accountable to high standards during GPT-4o’s rollout."
            }
          ],
          "analysis": "OpenAI has embedded ethical oversight into GPT-4o’s development lifecycle. The involvement of a dedicated Safety Advisory Group to double-check the model’s safety measures demonstrates strong internal accountability. Furthermore, OpenAI’s adherence to its published AI Charter and ongoing consultation with external experts mean there are checks and balances influencing how GPT-4o is refined and deployed. This governance approach helps ensure decisions about the model align with broader societal and ethical norms, not just technical or commercial considerations."
        },
        {
          "name": "Continuous Improvement",
          "kpi": "Evolution of safety practices",
          "value": "Active",
          "evidence_type": "Safety framework updates",
          "normalization": "Qualitative",
          "target": "Proactive updates are better",
          "evidence": [
            {
              "url": "https://openai.com/index/updating-our-preparedness-framework/",
              "title": "Our updated Preparedness Framework",
              "publish_date": "2025-04-15",
              "tier": "Tier 1",
              "quote": "We’re releasing an update to our Preparedness Framework... stronger requirements... clearer operational guidance on how we evaluate, govern, and disclose our safeguards.",
              "justification": "OpenAI regularly revises its safety and ethics methodologies in light of new insights. In April 2025, the company strengthened its Preparedness Framework for frontier AI, sharpening risk criteria and improving transparency in its safety governance. This demonstrates OpenAI’s willingness to continuously improve its ethical safeguards as AI capabilities advance."
            }
          ],
          "analysis": "OpenAI shows a clear commitment to learning and improving from both its own experience with GPT-4o and external feedback. The company has updated its safety frameworks and guidelines during GPT-4o’s lifecycle, indicating that it actively refines its ethical approaches over time. Additionally, OpenAI’s participation in industry initiatives (e.g., voluntary commitments to AI safety and cooperation with regulators) further exemplifies an ethical stance of responsibility. GPT-4o benefits from this environment of continuous ethical improvement."
        }
      ],
      "analysis": "In the Ethics pillar, OpenAI sets a high bar. The release of GPT-4o was accompanied by unprecedented transparency, including detailed system cards and risk disclosures. The model’s development was overseen by formal governance bodies (ensuring accountability), and OpenAI adheres to a public charter that prioritizes safety and societal benefit. Moreover, OpenAI continually updates its practices – for example, refining its Preparedness Framework to better anticipate and mitigate future risks. All these factors illustrate a strong ethical commitment to responsible AI deployment in the case of GPT-4o."
    }
  },
  "compliance_badges": [
    "SOC 2 Type 2 (Security & Privacy)",
    "Model System Card Published",
    "White House AI Safety Pledge (2023)"
  ],
  "analysis_summary": "OpenAI’s GPT-4o demonstrates exemplary attention to safety across all assessed pillars. The model is highly effective at filtering or refusing harmful content, with only minor lapses that OpenAI addressed rapidly. It shows improved neutrality on sensitive topics, reflecting deliberate bias mitigation efforts. OpenAI has implemented strong privacy protections (no training on user data by default, SOC 2 compliance) and robust security measures, as evidenced by GPT-4o’s resistance to jailbreaks and ongoing vulnerability monitoring. On the ethics front, OpenAI provides an uncommon level of transparency (publishing detailed system and safety reports) and involves external oversight in model deployment. Collectively, these practices result in an overall safety profile for GPT-4o that is among the best-in-class for current AI models, with proactive policies and mechanisms in place to minimize risks."
}